{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ssd\n",
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "def read_data():\n",
    "    #read in red wine data\n",
    "    urllib.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', 'winequality-red.csv')\n",
    "    with open('winequality-red.csv') as f:\n",
    "        lines = (line for line in f)\n",
    "        data = np.loadtxt(lines, delimiter=';', skiprows=1)\n",
    "    print data.shape\n",
    "    return data\n",
    "\n",
    "def knn(k, X_train, X_test, q_train):\n",
    "    \"\"\" k-nearest neighbors \"\"\"\n",
    " \n",
    "    # initialize list to store predicted class\n",
    "    pred_class = []\n",
    "    # for each instance in data testing,\n",
    "    # calculate distance in respect to data training\n",
    "    for ii, di in enumerate(X_test):\n",
    "        distances = []  # initialize list to store distance\n",
    "        for ij, dj in enumerate(X_train):\n",
    "            # calculate distances\n",
    "            distances.append((calc_dist(di,dj), ij))\n",
    "        # k-neighbors\n",
    "        k_nn = sorted(distances)[:k]\n",
    "        # predict the class for the instance\n",
    "        pred_class.append(classify(k_nn, q_train))\n",
    " \n",
    "    # return prediction class\n",
    "    return pred_class\n",
    " \n",
    "def calc_dist(di,dj):\n",
    "    \"\"\" Distance calculation for every\n",
    "        distance functions in use\"\"\"\n",
    "    return ssd.euclidean(di,dj) # built-in Euclidean fn\n",
    " \n",
    "def evaluate(result):\n",
    "    \"\"\" Evaluate the prediction class\"\"\"\n",
    " \n",
    "    # create eval result array to store evaluation result\n",
    "    eval_result = np.zeros(2,int)\n",
    "    for x in result:\n",
    "        # increment the correct prediction by 1\n",
    "        if x == 0:\n",
    "            eval_result[0] += 1\n",
    "        # increment the wrong prediction by 1\n",
    "        else:\n",
    "            eval_result[1] += 1\n",
    "    # return evaluation result\n",
    "    return eval_result\n",
    "\n",
    "def classify(k_nn, q_train):\n",
    "    \"\"\" Classify instance data test into class\"\"\"\n",
    " \n",
    "    qlabel = []\n",
    "    for dist, idx in k_nn:\n",
    "        # retrieve label class and store into qlabel\n",
    "        qlabel.append(q_train[idx])\n",
    " \n",
    "    # return prediction class\n",
    "    return np.argmax(np.bincount(qlabel))\n",
    " \n",
    "def main():\n",
    "    \"\"\" k-nearest neighbors classifier \"\"\"\n",
    "    # read dataset of red wine\n",
    "    print \"kur 1\"\n",
    "    urllib.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', 'winequality-red.csv')\n",
    "    with open('winequality-red.csv') as f:\n",
    "        lines = (line for line in f)\n",
    "        data = np.loadtxt(lines, delimiter=';', skiprows=1)\n",
    "    print data.shape\n",
    "    N = data.shape[0] #get tupple (numRows, numCols)\n",
    "    #np.random.shuffle(data)\n",
    "  \n",
    "    Nfolds = 10\n",
    "    sizes = np.tile(np.floor(N/10),(1,Nfolds))\n",
    "    sizes[-1] = sizes[-1] + N - sizes.sum()\n",
    "    c_sizes = np.hstack((0,np.cumsum(sizes)))\n",
    "    print c_sizes\n",
    "    X = np.copy(data[:,:11])# change to data here if you dont want to run the cv on independent test data\n",
    "    t = np.copy(data[:,11])\n",
    "    \n",
    "    # initialize K\n",
    "    K = [1,3,7,11,19]\n",
    "    cv_loss = np.zeros((Nfolds, len(K)))\n",
    "    \n",
    "    for i in range(len(K)):\n",
    "        for fold in range(Nfolds):\n",
    "            X_fold = X[c_sizes[fold]:c_sizes[fold+1],:]\n",
    "            X_train = np.delete(X,np.arange(c_sizes[fold],c_sizes[fold+1],1),0)\n",
    "\n",
    "            t_fold = t[c_sizes[fold]:c_sizes[fold+1]]\n",
    "            t_train = np.delete(t,np.arange(c_sizes[fold],c_sizes[fold+1],1),0)\n",
    "            \n",
    "            #print \"k-NN classification results for red wine data set:\"\n",
    "            #print\n",
    "            #print \"    Number of correct / wrong classified test records\"\n",
    "            #print \"k  | Euclidean dist |\"\n",
    "    \n",
    "            # predict the data test into class\n",
    "            pred_class = knn(K[i], X_train, X_fold, t_train)\n",
    "            # evaluate the predicted result\n",
    "            eval_result = evaluate(pred_class-t_fold)\n",
    "            # assign the evaluated result into classification result\n",
    "\n",
    "            # print the classification result into the screen\n",
    "            # print K[i], \" |     \", results[0], \"/\", results[1]\n",
    "            cv_loss[fold,i] = float(eval_result[1])/float(eval_result[0]+eval_result[1])\n",
    "        print \"current K,\",K[i]\n",
    "    \n",
    "    plt.plot(K,cv_loss.mean(axis=0),'r-',label=\"CV results\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Percent wrong classifications')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
